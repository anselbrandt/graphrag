{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import time\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setLevel(logging.INFO)\n",
    "\n",
    "logging.getLogger(\"httpx\").setLevel(logging.ERROR)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(message)s\",\n",
    "    handlers=[logging.FileHandler(\"logs.txt\"), stream_handler],\n",
    ")\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions.sentence_transformer_embedding_function import (\n",
    "    SentenceTransformerEmbeddingFunction,\n",
    ")\n",
    "from gliner import GLiNER\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from chunking_utils import get_chunks\n",
    "from llm_utils import ask_llm\n",
    "from metadata_utils import get_meta\n",
    "from nlp_utils import get_entities, get_tags, get_relevant_chunks\n",
    "from transcript_utils import srt_to_text\n",
    "\n",
    "\n",
    "class Entity(BaseModel):\n",
    "    start: int\n",
    "    end: int\n",
    "    text: str\n",
    "    label: str\n",
    "    score: float\n",
    "\n",
    "\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_base\", max_length=768)\n",
    "\n",
    "LLM_MODEL = \"qwen2.5:14b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromadb_dir = Path(\"chromadb\")\n",
    "chromadb_dir.mkdir(exist_ok=True)\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=str(chromadb_dir))\n",
    "collection = chroma_client.get_or_create_collection(name=\"roderick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    file for file in sorted(Path(\"../files/rotl\").iterdir()) if \".srt\" in file.name\n",
    "]\n",
    "\n",
    "for file in files[:1]:\n",
    "    file_name, episode_number, episode_date, episode_title = get_meta(file)\n",
    "    transcript = srt_to_text(file)\n",
    "    chunks = get_chunks(transcript)\n",
    "    start_entities = time.time()\n",
    "    results = get_entities(chunks, model)\n",
    "\n",
    "    logging.info(\n",
    "        f\"rotl {episode_number} entities generated {(time.time()-start_entities):.1f}s\"\n",
    "    )\n",
    "    for entity, data in results.items():\n",
    "        start_entity = time.time()\n",
    "        labels = data[\"labels\"]\n",
    "        indexes = data[\"indexes\"]\n",
    "        relevant_chunks = get_relevant_chunks(chunks, indexes)\n",
    "\n",
    "        context = \"\\n\".join(relevant_chunks)\n",
    "        question = f\"What do John and Merlin say about {entity}?\"\n",
    "        start_answer = time.time()\n",
    "        answer = ask_llm(f\"{context}\\n\\n{question}\", model=LLM_MODEL, tokens=500)\n",
    "\n",
    "        tags = get_tags(answer, model, stopwords=[\"john\", \"merlin\"])\n",
    "\n",
    "        doc = f\"{entity}\\n\\n{\", \".join(labels)}\\n\\n{\", \".join(tags)}\\n\\n{answer}\"\n",
    "\n",
    "        id = f\"{entity}_rotl_{episode_number}\"\n",
    "        metadata = {\n",
    "            \"chunks\": \",\".join([str(i) for i in indexes]),\n",
    "            \"show\": \"Roderick on the Line\",\n",
    "            \"episode\": episode_number,\n",
    "            \"title\": episode_title,\n",
    "            \"subject\": entity,\n",
    "            \"category\": \",\".join(labels),\n",
    "            \"tags\": \",\".join(tags),\n",
    "        }\n",
    "\n",
    "        collection.add(documents=[doc], ids=[id], metadatas=[metadata])\n",
    "        logging.info(\n",
    "            f\"rotl {episode_number} {entity} inserted {(time.time()-start_entity):.1f}s\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\n",
    "results = collection.query(query_texts=[query], n_results=10)\n",
    "\n",
    "docs = results[\"documents\"][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
