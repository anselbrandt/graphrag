{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Folder does not exist locally, attempting to use huggingface hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2426245db544ba0ae2934471d6fa844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ansel/ai/graphrag/.venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "import json\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import time\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setLevel(logging.INFO)\n",
    "\n",
    "logging.getLogger(\"httpx\").setLevel(logging.ERROR)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(message)s\",\n",
    "    handlers=[logging.FileHandler(\"logs.txt\"), stream_handler],\n",
    ")\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions.sentence_transformer_embedding_function import (\n",
    "    SentenceTransformerEmbeddingFunction,\n",
    ")\n",
    "from gliner import GLiNER\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from chunking_utils import get_chunks\n",
    "from llm_utils import ask_llm\n",
    "from metadata_utils import get_meta\n",
    "from nlp_utils import get_entities, get_tags, get_relevant_chunks\n",
    "from transcript_utils import srt_to_text\n",
    "\n",
    "\n",
    "class Entity(BaseModel):\n",
    "    start: int\n",
    "    end: int\n",
    "    text: str\n",
    "    label: str\n",
    "    score: float\n",
    "\n",
    "\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_base\", max_length=768)\n",
    "\n",
    "LLM_MODEL = \"qwen2.5:14b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "chromadb_dir = Path(\"chromadb\")\n",
    "chromadb_dir.mkdir(exist_ok=True)\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=str(chromadb_dir))\n",
    "collection = chroma_client.get_or_create_collection(name=\"roderick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rotl 000 entities generated 8.2s\n",
      "rotl 000 AK-Forty-Sevens 3.9s\n",
      "rotl 000 amazon.com 2.2s\n",
      "rotl 000 America 3.1s\n",
      "rotl 000 American 3.0s\n",
      "rotl 000 Americans 2.4s\n",
      "rotl 000 Arcade Fire 3.7s\n",
      "rotl 000 Arkansas 1.8s\n",
      "rotl 000 Arnold Palmer 2.1s\n",
      "rotl 000 Arnold Palmer drink 3.5s\n",
      "rotl 000 Arnold Palmer Light 5.0s\n",
      "rotl 000 Arnold Palmers 3.9s\n",
      "rotl 000 Artisanal 3.1s\n",
      "rotl 000 Artisanal bread 2.2s\n",
      "rotl 000 artisanal ones 2.3s\n",
      "rotl 000 baby 4.7s\n",
      "rotl 000 bachelor 2.3s\n",
      "rotl 000 band 2.8s\n",
      "rotl 000 bar 3.8s\n",
      "rotl 000 barrow smith 2.7s\n",
      "rotl 000 bars 3.0s\n",
      "rotl 000 bartender 5.3s\n",
      "rotl 000 battle 4.3s\n",
      "rotl 000 beer 2.1s\n",
      "rotl 000 Berlin 2.7s\n",
      "rotl 000 Bible 1.9s\n",
      "rotl 000 birthday party 2.1s\n",
      "rotl 000 black light 2.4s\n",
      "rotl 000 blend 3.9s\n",
      "rotl 000 book 2.0s\n",
      "rotl 000 burlap 2.4s\n",
      "rotl 000 button-down shirt 3.6s\n",
      "rotl 000 carbohydrates 3.1s\n",
      "rotl 000 Carson 1.5s\n",
      "rotl 000 cartoon character 2.2s\n",
      "rotl 000 CFLs 3.0s\n",
      "rotl 000 chamber 2.3s\n",
      "rotl 000 cheese 2.4s\n",
      "rotl 000 cherry juice 3.2s\n",
      "rotl 000 chick magnet trucker cap 2.7s\n",
      "rotl 000 child 4.7s\n",
      "rotl 000 children 3.0s\n",
      "rotl 000 China 2.3s\n",
      "rotl 000 Chris Walla 2.3s\n",
      "rotl 000 Christian 1.3s\n",
      "rotl 000 Christmas morning 2.9s\n",
      "rotl 000 Clark 2.5s\n",
      "rotl 000 Clark shoes 2.7s\n",
      "rotl 000 Clarks 3.0s\n",
      "rotl 000 clothes 3.0s\n",
      "rotl 000 Coca-Cola 3.2s\n",
      "rotl 000 coffee 2.5s\n",
      "rotl 000 Coke 2.4s\n",
      "rotl 000 Colin 3.8s\n",
      "rotl 000 Colton 2.1s\n",
      "rotl 000 concentrate 2.6s\n",
      "rotl 000 Converse 2.0s\n",
      "rotl 000 copper 2.0s\n",
      "rotl 000 corn 4.0s\n",
      "rotl 000 cotton 3.2s\n",
      "rotl 000 cowboy boots 2.1s\n",
      "rotl 000 cranberry juice 2.0s\n",
      "rotl 000 Creeper Lagoon 2.3s\n",
      "rotl 000 cute girl 1.9s\n",
      "rotl 000 dad 3.6s\n",
      "rotl 000 dad jeans 2.2s\n",
      "rotl 000 daughter 4.3s\n",
      "rotl 000 Dave Bazan 2.7s\n",
      "rotl 000 day 3.7s\n",
      "rotl 000 Death Cab for Cutie 2.6s\n",
      "rotl 000 Decembrist 2.8s\n",
      "rotl 000 Def Leppard 2.8s\n",
      "rotl 000 Dexter and Hayes 2.8s\n",
      "rotl 000 diapers 1.9s\n",
      "rotl 000 Disney 3.1s\n",
      "rotl 000 documentary 2.8s\n",
      "rotl 000 Donald Duck 2.5s\n",
      "rotl 000 door 2.3s\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "    file for file in sorted(Path(\"../files/rotl\").iterdir()) if \".srt\" in file.name\n",
    "]\n",
    "\n",
    "for file in files[:1]:\n",
    "    start_episode = time.time()\n",
    "\n",
    "    file_name, episode_number, episode_date, episode_title = get_meta(file)\n",
    "    transcript = srt_to_text(file)\n",
    "    chunks = get_chunks(transcript)\n",
    "    metadatas = [\n",
    "        {\n",
    "            \"chunks\": str(i),\n",
    "            \"show\": \"Roderick on the Line\",\n",
    "            \"episode\": episode_number,\n",
    "            \"title\": episode_title,\n",
    "            \"subject\": \"\",\n",
    "            \"category\": \"\",\n",
    "            \"tags\": \"\",\n",
    "        }\n",
    "        for i in range(len(chunks))\n",
    "    ]\n",
    "    ids = [f\"rotl_{episode_number}_{str(i)}\" for i in range(len(chunks))]\n",
    "\n",
    "    collection.add(documents=chunks, metadatas=metadatas, ids=ids)\n",
    "\n",
    "    results = get_entities(chunks, model)\n",
    "\n",
    "    logging.info(\n",
    "        f\"rotl {episode_number} entities generated {(time.time() - start_episode):.1f}s\"\n",
    "    )\n",
    "    for entity, data in results.items():\n",
    "        start_entity = time.time()\n",
    "        labels = data[\"labels\"]\n",
    "        indexes = data[\"indexes\"]\n",
    "        relevant_chunks = get_relevant_chunks(chunks, indexes)\n",
    "\n",
    "        context = \"\\n\".join(relevant_chunks)\n",
    "        question = f\"What do John and Merlin say about {entity}?\"\n",
    "        start_answer = time.time()\n",
    "        answer = ask_llm(f\"{context}\\n\\n{question}\", model=LLM_MODEL, tokens=500)\n",
    "\n",
    "        tags = get_tags(answer, model, stopwords=[\"john\", \"merlin\"])\n",
    "\n",
    "        doc = f\"{entity}\\n\\n{', '.join(labels)}\\n\\n{', '.join(tags)}\\n\\n{answer}\"\n",
    "\n",
    "        id = f\"{entity}_rotl_{episode_number}\"\n",
    "        metadata = {\n",
    "            \"chunks\": \",\".join([str(i) for i in indexes]),\n",
    "            \"show\": \"Roderick on the Line\",\n",
    "            \"episode\": episode_number,\n",
    "            \"title\": episode_title,\n",
    "            \"subject\": entity,\n",
    "            \"category\": \",\".join(labels),\n",
    "            \"tags\": \",\".join(tags),\n",
    "        }\n",
    "\n",
    "        collection.add(documents=[doc], ids=[id], metadatas=[metadata])\n",
    "        logging.info(\n",
    "            f\"rotl {episode_number} {entity} {(time.time() - start_entity):.1f}s\"\n",
    "        )\n",
    "    td = timedelta(seconds=time.time() - start_episode)\n",
    "    formatted = f\"{td.seconds // 60}min {td.seconds % 60}s\"\n",
    "    logging.info(f\"rotl {episode_number} {episode_title} {formatted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\n",
    "results = collection.query(query_texts=[query], n_results=10)\n",
    "\n",
    "docs = results[\"documents\"][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
