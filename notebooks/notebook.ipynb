{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metadata_utils import get_meta\n",
    "from llm_utils import ask_llm\n",
    "from transcript_utils import srt_to_text\n",
    "from chunking_utils import chunker, token_counter\n",
    "from chonkie import Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "files = [file for file in Path(\"../files/rotl\").iterdir() if \".srt\" in file.name]\n",
    "\n",
    "results_dir = Path(\"summaries_test\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "chunks_dir = Path(\"chunks\")\n",
    "chunks_dir.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "def who_is(subject, exclude=[]):\n",
    "    results = []\n",
    "    for file in files:\n",
    "        file_name, episode_number, episode_date, episode_title = get_meta(file)\n",
    "        text = srt_to_text(file)\n",
    "        if subject in text and all(name not in text for name in exclude):\n",
    "            chunks = chunker(text)\n",
    "            for chunk in chunks:\n",
    "                if isinstance(chunk, Chunk) and chunk.text.count(subject):\n",
    "                    results.append(chunk.text)\n",
    "    chunks_path = chunks_dir / f\"{subject}_chunks.txt\"\n",
    "    with open(chunks_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(results))\n",
    "    question = f\"Who is {subject}?\"\n",
    "    context = f\"{\"\\n\".join(results)}\\n\\n\\n{question}\"\n",
    "    raw_answer = ask_llm(context)\n",
    "    answer = raw_answer.strip().replace(\". \", \".\\n\")\n",
    "    out_path = results_dir / f\"{subject}.txt\"\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(answer)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eleanor is John Roderick's wife.\n",
      "\n",
      "In the story, John and Eleanor are on a flight together, and John is trying to navigate the airport using a sign with a map on it.\n",
      "Eleanor is worried about being late and doesn't want John to draw attention to themselves by trying to navigate.\n",
      "\n",
      "John compares Eleanor to his mother, who is always punctual and never late.\n",
      "He also mentions that Eleanor has been on the same flight for eight hours, which may explain her anxiety about being late.\n",
      "\n",
      "Later in the story, John mentions that Eleanor forbade him from feeding the cats because there were too many of them and they were causing problems.\n",
      "This suggests that Eleanor is a practical and no-nonsense person who is concerned about taking care of their home and family.\n",
      "\n",
      "Overall, Eleanor is portrayed as a loving and caring wife who is also practical and concerned about being on time.\n",
      "She is not afraid to stand up to her husband when she thinks he is being foolish, but she also cares about him and wants to protect him from embarrassment.\n"
     ]
    }
   ],
   "source": [
    "exclude = [\"Eleanor Rigby\", \"Eleanor Roosevelt\", \"Eleanor Braun\"]\n",
    "answer = who_is(\"Eleanor\", exclude=exclude)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75289d0aed15484799f9f9eba8089d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ansel/ai/graphrag/.venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from gliner import GLiNER\n",
    "\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_medium-v2.1\", max_length=768).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "files = [file for file in Path(\"../files/rotl\").iterdir() if \".srt\" in file.name]\n",
    "\n",
    "labels = [\"Person\"]\n",
    "\n",
    "STOP_WORDS = [\n",
    "    \"merlin\",\n",
    "    \"john\",\n",
    "    \"i\",\n",
    "    \"we\",\n",
    "    \"you\",\n",
    "    \"he\",\n",
    "    \"her\",\n",
    "    \"she\",\n",
    "    \"it\",\n",
    "    \"they\",\n",
    "    \"one\",\n",
    "    \"who\",\n",
    "    \"what\",\n",
    "    \"which\",\n",
    "]\n",
    "\n",
    "\n",
    "def get_names(file):\n",
    "    results = []\n",
    "    file_name, episode_number, episode_date, episode_title = get_meta(file)\n",
    "    transcript = srt_to_text(file)\n",
    "    chunks = chunker(transcript)\n",
    "    for chunk in chunks:\n",
    "        entities = model.predict_entities(chunk.text, labels, threshold=0.5)\n",
    "        for entity in entities:\n",
    "            if entity[\"text\"].lower() not in STOP_WORDS:\n",
    "                results.append(entity[\"text\"])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_dir = Path(\"names\")\n",
    "names_dir.mkdir(exist_ok=True)\n",
    "for file in files:\n",
    "    names = get_names(file)\n",
    "    names_path = names_dir / f\"{file.stem}.txt\"\n",
    "    with open(names_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def count_occurrences(nested_lists):\n",
    "    flat_list = [\n",
    "        item for sublist in nested_lists for item in sublist\n",
    "    ]  # Flatten the list\n",
    "    return dict(Counter(flat_list))\n",
    "\n",
    "\n",
    "name_files = [file for file in Path(\"names\").iterdir()]\n",
    "\n",
    "all_names = []\n",
    "\n",
    "for file in name_files:\n",
    "    with open(file, \"r\") as f:\n",
    "        names = f.read().splitlines()\n",
    "        all_names.append(names)\n",
    "\n",
    "name_counts = count_occurrences(all_names)\n",
    "\n",
    "counts_list = [(name, count) for name, count in name_counts.items()]\n",
    "\n",
    "sorted_counts = sorted(counts_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "counts_strings = [f\"{name}|{count}\" for name, count in sorted_counts]\n",
    "\n",
    "with open(\"counts.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(counts_strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12833"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"counts.txt\", \"r\") as f:\n",
    "    lines = f.read().splitlines()\n",
    "names = [line.split(\"|\")[0] for line in lines]\n",
    "\n",
    "len(names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
